{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b0e45",
   "metadata": {},
   "source": [
    "# Q1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6236a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Rank  \\\n",
      "0                             \"Baby Shark Dance\"[7]   \n",
      "1                                   \"Despacito\"[10]   \n",
      "2                        \"Johny Johny Yes Papa\"[18]   \n",
      "3                                   \"Bath Song\"[19]   \n",
      "4                               \"See You Again\"[20]   \n",
      "5                                \"Shape of You\"[25]   \n",
      "6                           \"Wheels on the Bus\"[28]   \n",
      "7                 \"Phonics Song with Two Words\"[29]   \n",
      "8                                 \"Uptown Funk\"[30]   \n",
      "9                               \"Gangnam Style\"[31]   \n",
      "10  \"Learning Colors – Colorful Eggs on a Farm\"[36]   \n",
      "11                             \"Dame Tu Cosita\"[37]   \n",
      "12   \"Masha and the Bear – Recipe for Disaster\"[38]   \n",
      "13                                     \"Axel F\"[39]   \n",
      "14                        \"Baa Baa Black Sheep\"[40]   \n",
      "15                                      \"Sugar\"[41]   \n",
      "16                             \"Lakdi Ki Kathi\"[42]   \n",
      "17                             \"Counting Stars\"[43]   \n",
      "18                                       \"Roar\"[44]   \n",
      "19           \"Waka Waka (This Time for Africa)\"[45]   \n",
      "20                      \"Shree Hanuman Chalisa\"[46]   \n",
      "21          \"Humpty the train on a fruits ride\"[47]   \n",
      "22                                      \"Sorry\"[48]   \n",
      "23                          \"Thinking Out Loud\"[49]   \n",
      "24                                    \"Perfect\"[50]   \n",
      "25                                 \"Dark Horse\"[51]   \n",
      "26                                 \"Let Her Go\"[52]   \n",
      "27                                      \"Faded\"[53]   \n",
      "28                             \"Girls Like You\"[54]   \n",
      "29                                    \"Lean On\"[55]   \n",
      "\n",
      "                                                 Name Artist  \\\n",
      "0         Pinkfong Baby Shark - Kids' Songs & Stories          \n",
      "1                                          Luis Fonsi          \n",
      "2   LooLoo Kids - Nursery Rhymes and Children's Songs          \n",
      "3                          Cocomelon - Nursery Rhymes          \n",
      "4                                         Wiz Khalifa          \n",
      "5                                          Ed Sheeran          \n",
      "6                          Cocomelon - Nursery Rhymes          \n",
      "7               ChuChu TV Nursery Rhymes & Kids Songs          \n",
      "8                                         Mark Ronson          \n",
      "9                                                 Psy          \n",
      "10                                        Miroshka TV          \n",
      "11                                      Ultra Records          \n",
      "12                                         Get Movies          \n",
      "13                                         Crazy Frog          \n",
      "14                         Cocomelon - Nursery Rhymes          \n",
      "15                                           Maroon 5          \n",
      "16                                       Jingle Toons          \n",
      "17                                        OneRepublic          \n",
      "18                                         Katy Perry          \n",
      "19                                            Shakira          \n",
      "20                              T-Series Bhakti Sagar          \n",
      "21      Kiddiestv Hindi - Nursery Rhymes & Kids Songs          \n",
      "22                                      Justin Bieber          \n",
      "23                                         Ed Sheeran          \n",
      "24                                         Ed Sheeran          \n",
      "25                                         Katy Perry          \n",
      "26                                          Passenger          \n",
      "27                                        Alan Walker          \n",
      "28                                           Maroon 5          \n",
      "29                               Major Lazer Official          \n",
      "\n",
      "          Upload Date  Views  \n",
      "0       June 17, 2016  14.52  \n",
      "1    January 12, 2017   8.44  \n",
      "2     October 8, 2016   6.91  \n",
      "3         May 2, 2018   6.70  \n",
      "4       April 6, 2015   6.27  \n",
      "5    January 30, 2017   6.26  \n",
      "6        May 24, 2018   6.13  \n",
      "7       March 6, 2014   5.80  \n",
      "8   November 19, 2014   5.23  \n",
      "9       July 15, 2012   5.15  \n",
      "10  February 27, 2018   5.11  \n",
      "11      April 5, 2018   4.63  \n",
      "12   January 31, 2012   4.58  \n",
      "13      June 16, 2009   4.54  \n",
      "14      June 25, 2018   4.05  \n",
      "15   January 14, 2015   4.05  \n",
      "16      June 14, 2018   4.04  \n",
      "17       May 31, 2013   4.02  \n",
      "18  September 5, 2013   4.00  \n",
      "19       June 4, 2010   3.93  \n",
      "20       May 10, 2011   3.84  \n",
      "21   January 26, 2018   3.81  \n",
      "22   October 22, 2015   3.80  \n",
      "23    October 7, 2014   3.77  \n",
      "24   November 9, 2017   3.74  \n",
      "25  February 20, 2014   3.72  \n",
      "26      July 25, 2012   3.66  \n",
      "27   December 3, 2015   3.63  \n",
      "28       May 31, 2018   3.61  \n",
      "29     March 22, 2015   3.60  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the Wikipedia page\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "\n",
    "# Extract video data\n",
    "videos = []\n",
    "table = driver.find_element(By.XPATH, '//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]')\n",
    "rows = table.find_elements(By.XPATH, './/tr')[1:]\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_elements(By.XPATH, './/td')\n",
    "    if len(cols) >= 5:\n",
    "        rank = cols[0].text.strip()\n",
    "        name_artist_text = cols[1].text.strip()\n",
    "        name, artist = name_artist_text.split('\\n') if '\\n' in name_artist_text else (name_artist_text, \"\")\n",
    "        upload_date = cols[3].text.strip()\n",
    "        views = cols[2].text.strip()\n",
    "        \n",
    "        videos.append({\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Artist\": artist,\n",
    "            \"Upload Date\": upload_date,\n",
    "            \"Views\": views\n",
    "        })\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Save the data to a DataFrame and then to a CSV file\n",
    "df = pd.DataFrame(videos)\n",
    "df.to_csv(\"most_viewed_youtube_videos.csv\", index=False)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8702049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbed6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47155b2e",
   "metadata": {},
   "source": [
    "# Q2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a420598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.bcci.tv/fixtures\")\n",
    "time.sleep(10) \n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        load_more_button = driver.find_element(By.XPATH, '//*[@id=\"fixtures\"]/div[3]/div[2]/div/button')\n",
    "        load_more_button.click()\n",
    "        time.sleep(5) \n",
    "    except:\n",
    "        break\n",
    "\n",
    "fixture_data = []\n",
    "\n",
    "\n",
    "\n",
    "for card in fixture_cards:\n",
    "    try:\n",
    "        series = driver.find_element(By.XPATH, '//h5[@class=\"match-tournament-name ng-binding\"]').text\n",
    "    except:\n",
    "        series = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        place = driver.find_element(By.XPATH, './/div[@class=\"fixture-card-bottom\"]/span').text.strip()\n",
    "    except:\n",
    "        place = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        date = driver.find_element(By.XPATH, './/div[@class=\"fixture-card-date-time\"]/span[1]').text.strip()\n",
    "    except:\n",
    "        date = \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        time = driver.find_element(By.XPATH, './/div[@class=\"fixture-card-date-time\"]/span[2]').text.strip()\n",
    "    except:\n",
    "        time = \"N/A\"\n",
    "\n",
    "    fixture_data.append({\n",
    "        \"Series\": series,\n",
    "        \"Place\": place,\n",
    "        \"Date\": date,\n",
    "        \"Time\": time\n",
    "    })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(fixture_data)\n",
    "df.to_csv(\"bcci_international_fixtures.csv\", index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79360f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57387e4a",
   "metadata": {},
   "source": [
    "# Q 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23fd693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div[2]/div[2]/ul/li[1]/a\"}\n",
      "  (Session info: chrome=125.0.6422.142); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F15D1F52+60322]\n",
      "\t(No symbol) [0x00007FF7F154CEC9]\n",
      "\t(No symbol) [0x00007FF7F1407EBA]\n",
      "\t(No symbol) [0x00007FF7F1457676]\n",
      "\t(No symbol) [0x00007FF7F145773C]\n",
      "\t(No symbol) [0x00007FF7F149E967]\n",
      "\t(No symbol) [0x00007FF7F147C25F]\n",
      "\t(No symbol) [0x00007FF7F149BC80]\n",
      "\t(No symbol) [0x00007FF7F147BFC3]\n",
      "\t(No symbol) [0x00007FF7F1449617]\n",
      "\t(No symbol) [0x00007FF7F144A211]\n",
      "\tGetHandleVerifier [0x00007FF7F18E94AD+3301629]\n",
      "\tGetHandleVerifier [0x00007FF7F19336D3+3605283]\n",
      "\tGetHandleVerifier [0x00007FF7F1929450+3563680]\n",
      "\tGetHandleVerifier [0x00007FF7F1684326+790390]\n",
      "\t(No symbol) [0x00007FF7F155750F]\n",
      "\t(No symbol) [0x00007FF7F1553404]\n",
      "\t(No symbol) [0x00007FF7F1553592]\n",
      "\t(No symbol) [0x00007FF7F1542F9F]\n",
      "\tBaseThreadInitThunk [0x00007FF8067B257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF806FCAA48+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "time.sleep(7)\n",
    "\n",
    "try:\n",
    "    economy_link = driver.find_element(By.XPATH, '//*[@id=\"top\"]/div[2]/div[2]/button')\n",
    "    economy_link.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    indian_economy_link = driver.find_element(By.XPATH, '//*[@id=\"top\"]/div[2]/div[2]/div/a[3]')\n",
    "    indian_economy_link.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    indian_states_link = driver.find_element(By.XPATH, '/html/body/div/div[2]/div[2]/ul/li[1]/a')\n",
    "    indian_states_link.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    try:\n",
    "        close_button = driver.find_element(By.XPATH, '//div[@id=\"dismiss-button\"]')\n",
    "        close_button.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    table = driver.find_element(By.XPATH, '//*[@id=\"table_id\"]/tbody')\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    gdp_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(data) == 6:\n",
    "            rank = data[0].text\n",
    "            state = data[1].text\n",
    "            gsdp_18_19 = data[2].text\n",
    "            gsdp_19_20 = data[3].text\n",
    "            share_18_19 = data[4].text\n",
    "            gdp_billion = data[5].text\n",
    "\n",
    "            gdp_data.append({\n",
    "                \"Rank\": rank,\n",
    "                \"State\": state,\n",
    "                \"GSDP(18-19) - Current Prices\": gsdp_18_19,\n",
    "                \"GSDP(19-20) - Current Prices\": gsdp_19_20,\n",
    "                \"Share(18-19)\": share_18_19,\n",
    "                \"GDP ($ Billion)\": gdp_billion\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(gdp_data)\n",
    "    df.to_csv(\"statewise_gdp_india.csv\", index=False)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddea91c",
   "metadata": {},
   "source": [
    "# Q 4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eeba646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://github.com/\")\n",
    "time.sleep(5)  # Wait for the page to fully load\n",
    "\n",
    "try:\n",
    " \n",
    "    trending_repos_section = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "    repos = trending_repos_section.find_elements(By.XPATH, './/article[@class=\"Box-row\"]')\n",
    "\n",
    "    repo_data = []\n",
    "\n",
    "    for repo in repos:\n",
    "        try:\n",
    "            title = repo.find_element(By.TAG_NAME, 'h1').text.strip()\n",
    "        except:\n",
    "            title = \"No title\"\n",
    "\n",
    "        try:\n",
    "            description = repo.find_element(By.TAG_NAME, 'p').text.strip()\n",
    "        except:\n",
    "            description = \"No description\"\n",
    "\n",
    "        try:\n",
    "            language = repo.find_element(By.XPATH, './/span[@itemprop=\"programmingLanguage\"]').text.strip()\n",
    "        except:\n",
    "            language = \"No language\"\n",
    "\n",
    "        try:\n",
    "            contributors = repo.find_element(By.XPATH, './/a[contains(@href, \"/contributors\")]').text.strip()\n",
    "        except:\n",
    "            contributors = \"0\"\n",
    "\n",
    "        repo_data.append({\n",
    "            \"Repository Title\": title,\n",
    "            \"Repository Description\": description,\n",
    "            \"Contributors Count\": contributors,\n",
    "            \"Language Used\": language\n",
    "        })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    " \n",
    "    df = pd.DataFrame(repo_data)\n",
    "    df.to_csv(\"github_trending_repos.csv\", index=False)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ce77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91608e",
   "metadata": {},
   "source": [
    "# Q 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c727795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "try:\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.billboard.com/charts/hot-100/\")\n",
    "    time.sleep(5)  \n",
    "\n",
    "\n",
    "    songs = driver.find_elements(By.XPATH, '//li[@class=\"chart-list__element display--flex\"]')\n",
    "\n",
    "    song_data = []\n",
    "\n",
    "    for song in songs[:100]:  \n",
    "        try:\n",
    "            song_name = song.find_element(By.XPATH, './/span[@class=\"chart-element__information\"]/span[@class=\"chart-element__information__song text--truncate color--primary\"]').text.strip()\n",
    "        except:\n",
    "            song_name = \"No Song Name\"\n",
    "\n",
    "        try:\n",
    "            artist_name = song.find_element(By.XPATH, './/span[@class=\"chart-element__information\"]/span[@class=\"chart-element__information__artist text--truncate color--secondary\"]').text.strip()\n",
    "        except:\n",
    "            artist_name = \"No Artist Name\"\n",
    "\n",
    "        try:\n",
    "            last_week_rank = song.find_element(By.XPATH, './/div[@class=\"chart-element__meta text--center color--secondary text--last\"]').text.strip()\n",
    "        except:\n",
    "            last_week_rank = \"No Last Week Rank\"\n",
    "\n",
    "        try:\n",
    "            peak_rank = song.find_element(By.XPATH, './/div[@class=\"chart-element__meta text--center color--secondary text--peak\"]').text.strip()\n",
    "        except:\n",
    "            peak_rank = \"No Peak Rank\"\n",
    "\n",
    "        try:\n",
    "            weeks_on_board = song.find_element(By.XPATH, './/div[@class=\"chart-element__meta text--center color--secondary text--week\"]').text.strip()\n",
    "        except:\n",
    "            weeks_on_board = \"No Weeks on Board\"\n",
    "\n",
    "        song_data.append({\n",
    "            \"Song Name\": song_name,\n",
    "            \"Artist Name\": artist_name,\n",
    "            \"Last Week Rank\": last_week_rank,\n",
    "            \"Peak Rank\": peak_rank,\n",
    "            \"Weeks on Board\": weeks_on_board\n",
    "        })\n",
    "\n",
    "  \n",
    "    driver.quit()\n",
    "\n",
    " \n",
    "    df = pd.DataFrame(song_data)\n",
    "    df.to_csv(\"billboard_top_100_songs.csv\", index=False)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf52c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269cc4b4",
   "metadata": {},
   "source": [
    "# Q 6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0e3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "   \n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//table[@class=\"in-article sortable\"]')))\n",
    "\n",
    "\n",
    "    table = driver.find_element(By.XPATH, '//table[@class=\"in-article sortable\"]')\n",
    "\n",
    "  \n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')[1:]  \n",
    "\n",
    "    book_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "        \n",
    "        \n",
    "        if len(columns) >= 6:\n",
    "            book_name = columns[1].text.strip()\n",
    "            author_name = columns[2].text.strip()\n",
    "            volumes_sold = columns[3].text.strip()\n",
    "            publisher = columns[4].text.strip()\n",
    "            genre = columns[5].text.strip()\n",
    "            \n",
    "           \n",
    "            book_data.append({\n",
    "                \"Book Name\": book_name,\n",
    "                \"Author Name\": author_name,\n",
    "                \"Volumes Sold\": volumes_sold,\n",
    "                \"Publisher\": publisher,\n",
    "                \"Genre\": genre\n",
    "            })\n",
    "\n",
    "   \n",
    "    driver.quit()\n",
    "\n",
    "   \n",
    "    df = pd.DataFrame(book_data)\n",
    "    df.to_csv(\"highest_selling_novels.csv\", index=False)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872dd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b95e2",
   "metadata": {},
   "source": [
    "# Q 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79d2fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "time.sleep(5)  \n",
    "\n",
    "try:\n",
    "\n",
    "    series_elements = driver.find_elements(By.XPATH, '//div[@class=\"lister-item-content\"]')\n",
    "\n",
    "    series_data = []\n",
    "\n",
    "    for series in series_elements:\n",
    "        try:\n",
    "            name = series.find_element(By.XPATH, './/h3[@class=\"lister-item-header\"]/a').text.strip()\n",
    "        except:\n",
    "            name = \"No Name\"\n",
    "\n",
    "        try:\n",
    "            year_span = series.find_element(By.XPATH, './/span[@class=\"lister-item-year text-muted unbold\"]').text.strip()\n",
    "        except:\n",
    "            year_span = \"No Year Span\"\n",
    "\n",
    "        try:\n",
    "            genre = series.find_element(By.XPATH, './/span[@class=\"genre\"]').text.strip()\n",
    "        except:\n",
    "            genre = \"No Genre\"\n",
    "\n",
    "        try:\n",
    "            runtime = series.find_element(By.XPATH, './/span[@class=\"runtime\"]').text.strip()\n",
    "        except:\n",
    "            runtime = \"No Runtime\"\n",
    "\n",
    "        try:\n",
    "            ratings = series.find_element(By.XPATH, './/div[@class=\"ipl-rating-star small\"]/span[@class=\"ipl-rating-star__rating\"]').text.strip()\n",
    "        except:\n",
    "            ratings = \"No Ratings\"\n",
    "\n",
    "        try:\n",
    "            votes = series.find_element(By.XPATH, './/p[@class=\"text-muted text-small\"]/span[@name=\"nv\"]').text.strip()\n",
    "        except:\n",
    "            votes = \"No Votes\"\n",
    "\n",
    "        series_data.append({\n",
    "            \"Name\": name,\n",
    "            \"Year Span\": year_span,\n",
    "            \"Genre\": genre,\n",
    "            \"Run Time\": runtime,\n",
    "            \"Ratings\": ratings,\n",
    "            \"Votes\": votes\n",
    "        })\n",
    "\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(series_data)\n",
    "    df.to_csv(\"imdb_most_watched_tv_series.csv\", index=False)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d29a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47df253",
   "metadata": {},
   "source": [
    "# Q 8.Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    " Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cb65eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)  \n",
    "\n",
    "try:\n",
    "   \n",
    "    show_all_link = driver.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a')\n",
    "    show_all_link.click()\n",
    "    time.sleep(3)  \n",
    "    table = driver.find_element(By.XPATH, '//div[@class=\"flex flex-col gap-1\"]')\n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')[1:]  \n",
    "\n",
    "    dataset_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "\n",
    "        \n",
    "        dataset_name = columns[0].text.strip()\n",
    "        data_type = columns[1].text.strip()\n",
    "        task = columns[2].text.strip()\n",
    "        attribute_types = columns[3].text.strip()\n",
    "        num_instances = columns[4].text.strip()\n",
    "        num_attributes = columns[5].text.strip()\n",
    "        year = columns[6].text.strip()\n",
    "\n",
    "      \n",
    "        dataset_data.append({\n",
    "            \"Dataset Name\": dataset_name,\n",
    "            \"Data Type\": data_type,\n",
    "            \"Task\": task,\n",
    "            \"Attribute Types\": attribute_types,\n",
    "            \"No of Instances\": num_instances,\n",
    "            \"No of Attributes\": num_attributes,\n",
    "            \"Year\": year\n",
    "        })\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "   \n",
    "    df = pd.DataFrame(dataset_data)\n",
    "    df.to_csv(\"uci_datasets_details.csv\", index=False)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c08f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
